{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "efficientnet_b4_acc_82%.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY0P8qCJXCCV"
      },
      "source": [
        "from google.colab import auth\r\n",
        "auth.authenticate_user()\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyETjOJJXgAR"
      },
      "source": [
        "from google.colab import output\r\n",
        "!cp \"/content/drive/MyDrive/YBIGTA/DACON/2차 배포.zip\" \"2차 배포.zip\"\r\n",
        "!unzip \"2차 배포.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXQInZkQXgUh"
      },
      "source": [
        "from google.colab import output\r\n",
        "!mkdir \"./dirty_mnist\"\r\n",
        "!unzip \"dirty_mnist_2nd.zip\" -d \"./dirty_mnist/\"\r\n",
        "!mkdir \"./test_dirty_mnist\"\r\n",
        "!unzip \"test_dirty_mnist_2nd.zip\" -d \"./test_dirty_mnist/\"\r\n",
        "output.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjhyKcTgTO4m"
      },
      "source": [
        "!pip install efficientnet_pytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWaNn2avXgaO"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import cv2\r\n",
        "import os\r\n",
        "from tqdm import tqdm\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch import nn\r\n",
        "from torchvision import models\r\n",
        "from torchvision import transforms\r\n",
        "from torch.utils.data import DataLoader, Dataset\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "from efficientnet_pytorch import EfficientNet\r\n",
        "\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # 디바이스 설정"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyBwXj2BXgeL"
      },
      "source": [
        "class DatasetMNIST(torch.utils.data.Dataset):\r\n",
        "    def __init__(self,\r\n",
        "                 dir_path,\r\n",
        "                 meta_df,\r\n",
        "                 transforms=None,\r\n",
        "                 augmentations=None):\r\n",
        "        \r\n",
        "        self.dir_path = dir_path # 데이터의 이미지가 저장된 디렉터리 경로\r\n",
        "        self.meta_df = meta_df # 데이터의 인덱스와 정답지가 들어있는 DataFrame\r\n",
        "\r\n",
        "        self.transforms = transforms\r\n",
        "        self.augmentations = augmentations\r\n",
        "        \r\n",
        "    def __len__(self):\r\n",
        "        return len(self.meta_df)\r\n",
        "    \r\n",
        "    def __getitem__(self, index):\r\n",
        "        # 폴더 경로 + 이미지 이름 + .png => 파일의 경로\r\n",
        "        # 참고) \"12\".zfill(5) => 00012\r\n",
        "        #       \"146\".zfill(5) => 00145\r\n",
        "        image = cv2.imread(self.dir_path +\\\r\n",
        "                           str(self.meta_df.iloc[index,0]).zfill(5) + '.png')\r\n",
        "\r\n",
        "        # transform 적용\r\n",
        "        if self.transforms:\r\n",
        "            image = self.transforms(image)\r\n",
        "\r\n",
        "        # 정답 numpy array생성\r\n",
        "        label = self.meta_df.iloc[index, 1:].values.astype('float')\r\n",
        "        label = torch.FloatTensor(label)\r\n",
        "\r\n",
        "        sample = {'image': image, 'label': label}\r\n",
        "\r\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sasqFhG3XgiO"
      },
      "source": [
        "dirty_mnist_answer = pd.read_csv(\"dirty_mnist_2nd_answer.csv\")\r\n",
        "# dirty_mnist라는 디렉터리 속에 들어있는 파일들의 이름을 namelist라는 변수에 저장\r\n",
        "namelist = os.listdir('./dirty_mnist/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZER1Rc4qTa6"
      },
      "source": [
        "transforms_train = transforms.Compose([\r\n",
        "    transforms.ToPILImage(),\r\n",
        "    transforms.RandomHorizontalFlip(),\r\n",
        "    transforms.RandomVerticalFlip(),\r\n",
        "    transforms.ToTensor(),\r\n",
        "    transforms.Normalize(\r\n",
        "        [0.485, 0.456, 0.406],\r\n",
        "        [0.229, 0.224, 0.225]\r\n",
        "    )\r\n",
        "])\r\n",
        "\r\n",
        "transforms_test = transforms.Compose([\r\n",
        "    transforms.ToTensor(),\r\n",
        "    transforms.Normalize(\r\n",
        "        [0.485, 0.456, 0.406],\r\n",
        "        [0.229, 0.224, 0.225]\r\n",
        "    )\r\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzDPtyPkSBqV"
      },
      "source": [
        "class EfficientNet_MultiLabel(nn.Module):\r\n",
        "    def __init__(self, in_channels):\r\n",
        "        super(EfficientNet_MultiLabel, self).__init__()\r\n",
        "        self.network = EfficientNet.from_pretrained('efficientnet-b4', in_channels=in_channels)\r\n",
        "        self.output_layer = nn.Linear(1000, 26)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = F.relu(self.network(x))\r\n",
        "        x = torch.sigmoid(self.output_layer(x))\r\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OqWM0WrnYRH"
      },
      "source": [
        "# cross validation을 적용하기 위해 KFold 생성\r\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\r\n",
        "epochs = 10\r\n",
        "batch_size = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqf9arDLXgo9"
      },
      "source": [
        "# dirty_mnist_answer에서 train_idx와 val_idx를 생성\r\n",
        "best_models = [] # 폴드별로 가장 validation acc가 높은 모델 저장\r\n",
        "\r\n",
        "for fold_index, (trn_idx, val_idx) in enumerate(kfold.split(dirty_mnist_answer), 1): # fold_index 1부터 시작\r\n",
        "    # 하나의 fold만 실행\r\n",
        "    if fold_index == 2:\r\n",
        "      break\r\n",
        "\r\n",
        "    print(f'[fold: {fold_index}]')\r\n",
        "    # cuda cache 초기화\r\n",
        "    torch.cuda.empty_cache()\r\n",
        "\r\n",
        "    # train fold, validation fold 분할\r\n",
        "    train_answer = dirty_mnist_answer.iloc[trn_idx]\r\n",
        "    test_answer  = dirty_mnist_answer.iloc[val_idx]\r\n",
        "\r\n",
        "    # Dataset 정의\r\n",
        "    train_dataset = DatasetMNIST(\"dirty_mnist/\", train_answer, transforms_train)\r\n",
        "    valid_dataset = DatasetMNIST(\"dirty_mnist/\", test_answer, transforms_test)\r\n",
        "\r\n",
        "    # DataLoader 정의\r\n",
        "    train_data_loader = DataLoader(\r\n",
        "        train_dataset,\r\n",
        "        batch_size = batch_size,\r\n",
        "        shuffle = False,\r\n",
        "        num_workers = 3\r\n",
        "    )\r\n",
        "    valid_data_loader = DataLoader(\r\n",
        "        valid_dataset,\r\n",
        "        batch_size = batch_size,\r\n",
        "        shuffle = False,\r\n",
        "        num_workers = 3\r\n",
        "    )\r\n",
        "\r\n",
        "    # 모델 선언\r\n",
        "    # gpu에 모델 할당\r\n",
        "    model = EfficientNet_MultiLabel(in_channels=3).to(device)\r\n",
        "\r\n",
        "    # 훈련 옵션 설정\r\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\r\n",
        "    decay_steps = (len(train_dataset) // batch_size) * epochs\r\n",
        "    scheduler_poly_lr_decay = PolynomialLRDecay(optimizer, max_decay_steps=decay_steps, end_learning_rate=1e-6, power=0.9)\r\n",
        "\r\n",
        "    criterion = torch.nn.BCELoss()\r\n",
        "\r\n",
        "    # 훈련 시작\r\n",
        "    valid_acc_max = 0\r\n",
        "    for epoch in range(epochs):\r\n",
        "        train_acc_list = []\r\n",
        "        with tqdm(train_data_loader, # train_data_loader를 iterative하게 반환\r\n",
        "                total=train_data_loader.__len__(), # train_data_loader의 크기\r\n",
        "                unit=\"batch\") as train_bar: # 한번 반환하는 sample의 단위는 \"batch\"\r\n",
        "            for sample in train_bar:\r\n",
        "                train_bar.set_description(f\"Train Epoch {epoch}\")\r\n",
        "                # 갱신할 변수들에 대한 모든 변화도를 0으로 초기화\r\n",
        "                optimizer.zero_grad()\r\n",
        "\r\n",
        "                images, labels = sample['image'], sample['label']\r\n",
        "                # tensor를 gpu에 올리기 \r\n",
        "                images = images.to(device)\r\n",
        "                labels = labels.to(device)\r\n",
        "\r\n",
        "                # 모델의 dropoupt, batchnormalization를 train 모드로 설정\r\n",
        "                model.train()\r\n",
        "                # .forward()에서 중간 노드의 gradient를 계산\r\n",
        "                with torch.set_grad_enabled(True):\r\n",
        "                    # 모델 예측\r\n",
        "                    probs  = model(images)\r\n",
        "                    # loss 계산\r\n",
        "                    loss = criterion(probs, labels)\r\n",
        "                    # 중간 노드의 gradient로 backpropagation을 적용하여 gradient 계산\r\n",
        "                    loss.backward()\r\n",
        "                    # weight 갱신\r\n",
        "                    optimizer.step()\r\n",
        "                    scheduler_poly_lr_decay.step()\r\n",
        "\r\n",
        "                    # train accuracy 계산\r\n",
        "                    probs  = probs.cpu().detach().numpy()\r\n",
        "                    labels = labels.cpu().detach().numpy()\r\n",
        "                    preds = probs > 0.5\r\n",
        "                    batch_acc = (labels == preds).mean()\r\n",
        "                    train_acc_list.append(batch_acc)\r\n",
        "                    train_acc = np.mean(train_acc_list)\r\n",
        "\r\n",
        "                # 현재 progress bar에 현재 미니배치의 loss 결과 출력\r\n",
        "                train_bar.set_postfix(train_loss= loss.item(),\r\n",
        "                                      train_acc = train_acc)\r\n",
        "                \r\n",
        "\r\n",
        "        # 1개 epoch학습 후 Validation 점수 계산\r\n",
        "        valid_acc_list = []\r\n",
        "        with tqdm(valid_data_loader,\r\n",
        "                total=valid_data_loader.__len__(),\r\n",
        "                unit=\"batch\") as valid_bar:\r\n",
        "            for sample in valid_bar:\r\n",
        "                valid_bar.set_description(f\"Valid Epoch {epoch}\")\r\n",
        "                optimizer.zero_grad()\r\n",
        "\r\n",
        "                images, labels = sample['image'], sample['label']\r\n",
        "                images = images.to(device)\r\n",
        "                labels = labels.to(device)\r\n",
        "\r\n",
        "                # 모델의 dropoupt, batchnormalization를 eval모드로 설정\r\n",
        "                model.eval()\r\n",
        "                # .forward()에서 중간 노드의 gradient를 계산\r\n",
        "                with torch.no_grad():\r\n",
        "                    # validation loss만을 계산\r\n",
        "                    probs  = model(images)\r\n",
        "                    valid_loss = criterion(probs, labels)\r\n",
        "\r\n",
        "                    # train accuracy 계산\r\n",
        "                    probs  = probs.cpu().detach().numpy()\r\n",
        "                    labels = labels.cpu().detach().numpy()\r\n",
        "                    preds = probs > 0.5\r\n",
        "                    batch_acc = (labels == preds).mean()\r\n",
        "                    valid_acc_list.append(batch_acc)\r\n",
        "\r\n",
        "                valid_acc = np.mean(valid_acc_list)\r\n",
        "                valid_bar.set_postfix(valid_loss = valid_loss.item(),\r\n",
        "                                      valid_acc = valid_acc)\r\n",
        "\r\n",
        "        # 모델 저장\r\n",
        "        if valid_acc_max < valid_acc:\r\n",
        "            valid_acc_max = valid_acc\r\n",
        "            best_model = model\r\n",
        "            MODEL = \"efficientnet-b4\"\r\n",
        "            path = \"/content/drive/MyDrive/YBIGTA/DACON/\"\r\n",
        "            torch.save(best_model, f'{path}{fold_index}_{MODEL}_{valid_loss.item():2.4f}_epoch_{epoch}.pth')\r\n",
        "\r\n",
        "    # 폴드별로 가장 좋은 모델 저장\r\n",
        "    best_models.append(best_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNgI8XRAXgr5"
      },
      "source": [
        "# 예측 테스트\r\n",
        "# gpu에 올라가 있는 tensor -> cpu로 이동 -> numpy array로 변환\r\n",
        "sample_images = images.cpu().detach().numpy()\r\n",
        "sample_prob = probs\r\n",
        "sample_labels = labels\r\n",
        "\r\n",
        "idx = 1\r\n",
        "plt.imshow(sample_images[idx][0])\r\n",
        "plt.title(\"sample input image\")\r\n",
        "plt.show()\r\n",
        "\r\n",
        "print('예측값 : ',dirty_mnist_answer.columns[1:][sample_prob[idx] > 0.5])\r\n",
        "print('정답값 : ', dirty_mnist_answer.columns[1:][sample_labels[idx] > 0.5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M49GpfYwXgvB"
      },
      "source": [
        "#test Dataset 정의\r\n",
        "sample_submission = pd.read_csv(\"sample_submission.csv\")\r\n",
        "test_dataset = DatasetMNIST(\"test_dirty_mnist/\", sample_submission, transforms_test)\r\n",
        "test_data_loader = DataLoader(\r\n",
        "    test_dataset,\r\n",
        "    batch_size = batch_size,\r\n",
        "    shuffle = False,\r\n",
        "    num_workers = 3\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q93Y4jlwXgyO"
      },
      "source": [
        "predictions_list = []\r\n",
        "# 배치 단위로 추론\r\n",
        "prediction_df = pd.read_csv(\"sample_submission.csv\")\r\n",
        "\r\n",
        "# fold마다 가장 좋은 모델을 이용하여 예측\r\n",
        "for model in best_models:\r\n",
        "    # 0으로 채워진 array 생성\r\n",
        "    prediction_array = np.zeros([prediction_df.shape[0],\r\n",
        "                                 prediction_df.shape[1] -1])\r\n",
        "    for idx, sample in enumerate(test_data_loader):\r\n",
        "        with torch.no_grad():\r\n",
        "            # 추론\r\n",
        "            model.eval()\r\n",
        "            images = sample['image']\r\n",
        "            images = images.to(device)\r\n",
        "            probs  = model(images)\r\n",
        "            probs = probs.cpu().detach().numpy()\r\n",
        "            preds = (probs > 0.5)\r\n",
        "\r\n",
        "            # 예측 결과를 prediction_array에 입력\r\n",
        "            batch_index = batch_size * idx\r\n",
        "            prediction_array[batch_index: batch_index + images.shape[0],:]\\\r\n",
        "                         = preds.astype(int)\r\n",
        "                         \r\n",
        "    # 채널을 하나 추가하여 list에 append\r\n",
        "    predictions_list.append(prediction_array[...,np.newaxis])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGYncWklXg1Q"
      },
      "source": [
        "# axis = 2를 기준으로 평균\r\n",
        "predictions_array = np.concatenate(predictions_list, axis = 2)\r\n",
        "predictions_mean = predictions_array.mean(axis = 2)\r\n",
        "\r\n",
        "# 평균 값이 0.5보다 클 경우 1 작으면 0\r\n",
        "predictions_mean = (predictions_mean > 0.5) * 1\r\n",
        "predictions_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypZo7C5fXg4a"
      },
      "source": [
        "sample_submission = pd.read_csv(\"sample_submission.csv\")\r\n",
        "sample_submission.iloc[:, 1:] = predictions_mean\r\n",
        "sample_submission.to_csv(\"efficientnet_b4.csv\", index = False)\r\n",
        "sample_submission"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETlHA1Z3Xg-r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}